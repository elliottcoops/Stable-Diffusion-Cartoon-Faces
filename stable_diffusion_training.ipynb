{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T15:17:17.300900Z",
     "iopub.status.busy": "2025-06-16T15:17:17.300616Z",
     "iopub.status.idle": "2025-06-16T15:17:19.252527Z",
     "shell.execute_reply": "2025-06-16T15:17:19.251790Z",
     "shell.execute_reply.started": "2025-06-16T15:17:17.300877Z"
    },
    "id": "uUTYR1C3jmzP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from unet_architecture import UNetWithTimeEmbedding\n",
    "from vae_architecture import SpatialVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T15:17:52.502744Z",
     "iopub.status.busy": "2025-06-16T15:17:52.501935Z",
     "iopub.status.idle": "2025-06-16T15:17:55.368294Z",
     "shell.execute_reply": "2025-06-16T15:17:55.367458Z",
     "shell.execute_reply.started": "2025-06-16T15:17:52.502718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Skiittoo/cartoon-faces\", split=\"train\")\n",
    "\n",
    "# preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)), # resize to 64x64 for vae\n",
    "    transforms.ToTensor(), # moves image in [0,1] range\n",
    "])\n",
    "\n",
    "class CartoonFacesDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        img, caption = item['image'], item['text']\n",
    "        \n",
    "        if img.mode == 'RGBA': # 4d to 3d image (sometimes used in hugging face)\n",
    "            img = img.convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, caption\n",
    "\n",
    "cartoon_dataset = CartoonFacesDataset(dataset, transform=transform)\n",
    "dataloader = DataLoader(cartoon_dataset, batch_size=32, shuffle=True) # load into dataloader for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vae architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:33:26.008128Z",
     "iopub.status.busy": "2025-06-16T10:33:26.007959Z",
     "iopub.status.idle": "2025-06-16T10:33:26.022856Z",
     "shell.execute_reply": "2025-06-16T10:33:26.022245Z",
     "shell.execute_reply.started": "2025-06-16T10:33:26.008114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def vae_loss(recon, x, z_mean, z_log_var):\n",
    "    recon_loss = nn.functional.binary_cross_entropy(recon, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.mean(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())  \n",
    "    return (recon_loss + kl_loss) / x.size(0)\n",
    "\n",
    "\n",
    "def show_reconstructions(vae, imgs, epoch, num=5, device='cuda'):\n",
    "    vae.eval()\n",
    "    imgs = imgs.to(device)\n",
    "    with torch.no_grad():\n",
    "        reconstructed, z_mean, z_log_var = vae(imgs)\n",
    "\n",
    "    imgs = imgs.detach().cpu()\n",
    "    reconstructed = reconstructed.detach().cpu()\n",
    "\n",
    "    fig, axs = plt.subplots(2, num, figsize=(2 * num, 4))\n",
    "    for i in range(num):\n",
    "        axs[0, i].imshow(imgs[i].permute(1, 2, 0).clip(0, 1))\n",
    "        axs[0, i].set_title(\"Original\")\n",
    "        axs[0, i].axis(\"off\")\n",
    "\n",
    "        axs[1, i].imshow(reconstructed[i].permute(1, 2, 0).clip(0, 1))\n",
    "        axs[1, i].set_title(\"Reconstructed\")\n",
    "        axs[1, i].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"Reconstructions after epoch {epoch}\")\n",
    "    plt.show()\n",
    "\n",
    "def train(vae, optimizer, epoch, loader, device='cuda'):\n",
    "    vae.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    imgs_to_show = None\n",
    "    \n",
    "    for imgs in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, z_mean, z_log_var = vae(imgs)\n",
    "        loss = vae_loss(reconstructed, imgs, z_mean, z_log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if imgs_to_show is None:\n",
    "            imgs_to_show = imgs.detach().cpu()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    print(f'Epoch {epoch} Loss {avg_loss}')\n",
    "\n",
    "    show_reconstructions(vae, imgs_to_show, epoch, num=5, device=device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vae = SpatialVAE()\n",
    "vae.to(device)\n",
    "# vae.load_state_dict(torch.load(\"/kaggle/working/vae_epoch30.pth\", map_location=device))\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 50\n",
    "# go from 1,...,epochs+1\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(vae, optimizer, epoch, dataloader, device)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(vae.state_dict(), f\"spatial_vae_epoch{epoch}.pth\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:33:26.024663Z",
     "iopub.status.busy": "2025-06-16T10:33:26.024448Z",
     "iopub.status.idle": "2025-06-16T10:33:26.039480Z",
     "shell.execute_reply": "2025-06-16T10:33:26.038972Z",
     "shell.execute_reply.started": "2025-06-16T10:33:26.024648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def show_reconstructions(originals, reconstructed, n=5):\n",
    "    originals = originals.detach().cpu().permute(0, 2, 3, 1)\n",
    "    reconstructed = reconstructed.detach().cpu().permute(0, 2, 3, 1)\n",
    "    fig, axs = plt.subplots(2, n, figsize=(2*n, 4))\n",
    "    for i in range(n):\n",
    "        axs[0, i].imshow(originals[i].clip(0, 1))\n",
    "        axs[0, i].axis(\"off\")\n",
    "        axs[1, i].imshow(reconstructed[i].clip(0, 1))\n",
    "        axs[1, i].axis(\"off\")\n",
    "    plt.suptitle(\"Top row: Original, Bottom row: Reconstructed\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Assuming you already have your VAE and DataLoader set up:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = SpatialVAE()\n",
    "vae.to(device)\n",
    "vae.load_state_dict(torch.load(\"/kaggle/input/stable_vae/pytorch/default/1/spatial_vae_epoch50.pth\", map_location=device))\n",
    "vae.eval()\n",
    "\n",
    "# Get the first batch from your DataLoader\n",
    "with torch.no_grad():\n",
    "    imgs = next(iter(dataloader))  # This gets first batch\n",
    "    imgs = imgs.to(device)\n",
    "    reconstructed, z_mean, z_log_var = vae(imgs)\n",
    "\n",
    "# Display side by side\n",
    "show_reconstructions(imgs, reconstructed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stable diffusion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T15:51:40.650561Z",
     "iopub.status.busy": "2025-06-16T15:51:40.649835Z",
     "iopub.status.idle": "2025-06-16T15:51:40.666725Z",
     "shell.execute_reply": "2025-06-16T15:51:40.665986Z",
     "shell.execute_reply.started": "2025-06-16T15:51:40.650533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def forward_diffusion_process(x0, t, beta_schedule):\n",
    "    beta = beta_schedule.to(x0.device)\n",
    "    alpha = 1 - beta\n",
    "    alpha_cumprod = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "    a_cumprod_t = alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "    a_cumprod_t = torch.clamp(a_cumprod_t, min=1e-8, max=1.0)\n",
    "    a_sqrts = torch.sqrt(a_cumprod_t)\n",
    "\n",
    "    one_minus_cumprod_t = 1 - a_cumprod_t\n",
    "    one_minus_cumprod_t = torch.clamp(one_minus_cumprod_t, min=1e-8, max=1.0)\n",
    "    one_minus_sqrts = torch.sqrt(one_minus_cumprod_t)\n",
    "\n",
    "    epsilon = torch.randn_like(x0)\n",
    "\n",
    "    noisy = a_sqrts * x0 + one_minus_sqrts * epsilon\n",
    "    return noisy, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = SpatialVAE()\n",
    "vae.to(device)\n",
    "vae.load_state_dict(torch.load(\"/kaggle/input/stable_vae/pytorch/default/1/spatial_vae_epoch50.pth\", map_location=device))\n",
    "vae.eval()\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T15:48:55.519714Z",
     "iopub.status.busy": "2025-06-16T15:48:55.519399Z",
     "iopub.status.idle": "2025-06-16T15:49:20.390170Z",
     "shell.execute_reply": "2025-06-16T15:49:20.389153Z",
     "shell.execute_reply.started": "2025-06-16T15:48:55.519692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "timesteps = 1000\n",
    "epochs = 15\n",
    "lr = 2e-4\n",
    "latent_channels = 4  # VAE latent channel dimension\n",
    "\n",
    "beta_schedule = linear_beta_schedule(timesteps).to(device)\n",
    "alpha = 1 - beta_schedule\n",
    "alpha_cumprod = torch.cumprod(alpha, dim=0)\n",
    "alpha_cumprod_prev = torch.cat([torch.ones(1, device=device), alpha_cumprod[:-1]])\n",
    "\n",
    "model = UNetWithTimeEmbedding(latent_channels=latent_channels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for images, captions in dataloader:  \n",
    "        images = images.to(device)\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        inputs = clip_processor(\n",
    "            images=images,\n",
    "            text=captions,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            do_rescale=False  # set depending on your image dtype\n",
    "        )\n",
    "        \n",
    "        # move all inputs to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # extract only the text inputs needed for clip_model.text_model()\n",
    "        text_inputs = {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"]\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_outputs = clip_model.text_model(**text_inputs)\n",
    "            text_features = text_outputs.pooler_output\n",
    "\n",
    "        # encode images with VAE (no grad)\n",
    "        with torch.no_grad():\n",
    "            mu, log_var = vae.encoder(images)\n",
    "            var = torch.exp(log_var).clamp(min=1e-6)\n",
    "            std = var.sqrt()\n",
    "            latents = mu + std * torch.randn_like(std)\n",
    "\n",
    "        # sample diffusion timestep for each sample in batch\n",
    "        t = torch.randint(0, timesteps, (batch_size,), device=device)\n",
    "\n",
    "        # forward diffusion \n",
    "        noisy_latents, noise = forward_diffusion_process(latents, t, beta_schedule)\n",
    "\n",
    "        # predict noise with UNet conditioned on text embeddings\n",
    "        noise_pred = model(noisy_latents, t, text=text_features)\n",
    "\n",
    "        loss = criterion(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / len(dataloader):.6f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"unet_weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 376944,
     "modelInstanceId": 355644,
     "sourceId": 436049,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 377641,
     "modelInstanceId": 356348,
     "sourceId": 436876,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31042,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
